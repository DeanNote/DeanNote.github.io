---
layout: single
title:  "AI기본용어 정리"
tag: [LLM, AI]
categories: ai
toc: true
author_profile: false
sidebar:
    nav: "docs"
#search: false 검색 불가능 포스팅
---

# 기술용어 정리
## LLM을 활용한 실전 AI 애플리케이션 개발

### LLM (Large Language Model)
- 기술적으로 **<u>딥러닝</u>**에 기반을 두고 있다.
- 사람의 언어를 컴퓨터가 이해하고 생성 할 수 있도록 연구하는 **<u>자연어 처리 분야</u>**에 속하며, 특히 그 중에서도 사람과 비슷하게 텍스트를 생성하는 방법을 연구하는 자연어 생성에 속한다.
- 다음에 올 단어가 무엇인지 예측하면서 문장을 하나씩 만들어 가는 방식으러 텍스트를 생성하는데, 이렇게 다음에 올 단어를 예측하는 모델을 **<u>언어 모델</u>**이라고 한다.
> 정리 : LLM은 딥러닝 기반의 언어모델이다.

### 임베딩 (embedding)
- 데이터의 의미와 특징을 포착해 **<u>숫자로 표현</u>**한 것 
  ##### 예시) 나의 `MBTI`가 `ISTJ`이고 각 점수가 `[0.5, 0.9, 0.7, 0.5]`이다.
- MBTI 검사 결과를 바탕으로 사람을 4개의 숫자로 표현한것 처럼, 데이터를 그 의미를 담아 여러 개의 숫자의 집합으로 표현하는 것을 말한다.<br> 
  ##### 숫자로 나타내면 좋은점
- 데이터를 임베딩으로 표현하면 데이터 사이의 거리를 계산하고 거리를 바탕으로 관련 있는 데이터와 관련이 없는 데이터를 구분할 수 있다.
  ##### 임베딩은 거리를 계산할 수 있기 때문에 다음과 같은 작업에 활용할 수 있다.
- **검색 및 추천** : 검색어와 관련이 있는 상품을 추천한다.
- **클러스터링 및 분류** : 유사하고 관련이 있는 데이터를 하나로 묶는다.
- **이상치**(outlier) **탐지** : 나머지 데이터와 거리가 먼 데이터는 이상치로 볼 수 있다.
> 2013년 구글에서 발표한 [「Efficient Estimation of Word Representations in Vector Space(백터 공간에서 단어 표현의 효율적인 추정」](https://arxiv.org/pdf/1301.3781.pdf)라는 논문에서는 워드투백(word2vec)이라는 모델을 통해 단어를 임베딩으로 변환하는 방법을 소개했다. 
- 예시와 같이 사람이 MBTI 검사를 통해 4개의 숫자로 표현되는 것처럼 단어가 워드투백 모델을 통해 숫자의 집합인 임베딩으로 변환된다. 이와 같이 단어를 임베딩으로 변환한 것을 일컬어 **<u>단어 임베딩 (word embedding</u>**)이라고 한다.

### 언어모델링
- 모델이 입력받은 텍스트의 다음 단어를 예측해 텍스트를 생성하는 방식을 말한다. 
- 다음 단어를 예측하는 방식으로 훈련한 모델을 언어 모델(language model)이라고 한다. 
- 텍스트를 생성하는 모델을 학습시키는 방법으로도 사용되지만, 대량의 데이터에서 언어의 특성을 학습하는 사전 학습 (pre-training)과제로도 많이 사용된다.
### 전이 학습 (transfer learning)
- 딥러닝 분야에서는 하나의 문제를 해결하는 과정에서 얻은 지식과 정보를 다른 문제를 풀 때 사용하는 방식

| 사전학습| 미세조정 (fine-tuning) | 
| --- | --- | 
| 대량의 데이터로 모델을 학습 | 특정한 문제를 해결하기 위한 데이터로 추가 학습 | 

### RAG
### 트랜스포머 아키텍처

  


 
